{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Convolutional Networks for text Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_excel(\"data2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = {}\n",
    "def get_sample_text(sample):\n",
    "    assert sample['column'][3]['@name'] == 'text'\n",
    "    return sample['column'][3]['#text']\n",
    "\n",
    "\n",
    "def get_sample_answers_bank(sample):\n",
    "    answers = {}\n",
    "    for i in range(4, 12):\n",
    "        companies[sample['column'][i]['@name']] = i\n",
    "        answers[sample['column'][i]['@name']] = None if sample['column'][i]['#text'] == 'NULL'\\\n",
    "            else int(sample['column'][i]['#text'])\n",
    "    return answers\n",
    "\n",
    "def get_sample_answers_tkk(sample):\n",
    "    answers = {}\n",
    "    for i in range(4, 11):\n",
    "        companies[sample['column'][i]['@name']] = i\n",
    "        answers[sample['column'][i]['@name']] = None if sample['column'][i]['#text'] == 'NULL'\\\n",
    "            else int(sample['column'][i]['#text'])\n",
    "    return answers\n",
    "\n",
    "def get_sample_id(sample):\n",
    "    assert sample['column'][0]['@name'] == 'id'\n",
    "    return int(sample['column'][0]['#text'])\n",
    "\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.DataFrame()\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        d = xmltodict.parse(f.read(), process_namespaces=True)\n",
    "        clean_samples = []\n",
    "        for sample in d['pma_xml_export']['database']['table']:\n",
    "            sample_id = get_sample_id(sample)\n",
    "            text = get_sample_text(sample)\n",
    "            answers = get_sample_answers_bank(sample)\n",
    "            for company, answer in answers.items():\n",
    "                if answer is not None:\n",
    "                    clean_samples.append((sample_id, text, company, answer))\n",
    "        df['text'] = [sample[1] for sample in clean_samples]\n",
    "        df['answer'] = [sample[3] for sample in clean_samples]\n",
    "        df['company'] = [sample[2] for sample in clean_samples]\n",
    "        df['sample_id'] = [sample[0] for sample in clean_samples]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE \n",
    "import xmltodict\n",
    "import re\n",
    "\n",
    "train_filename = \"bank_train_2016.xml\"\n",
    "test_filename = \"banks_test_etalon.xml\"\n",
    "\n",
    "train = get_data(train_filename)\n",
    "test = get_data(test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_replacement = lambda x: re.sub(r'(?:http[^\\s]+)($|\\s)', r'url\\1', x)\n",
    "user_replacement = lambda x: re.sub(r'(?:@[^\\s]+)($|\\s)', r'user\\1', x)\n",
    "\n",
    "train['text'] = train['text'].apply(url_replacement)\n",
    "train['text'] = train['text'].apply(user_replacement)\n",
    "\n",
    "test['text'] = test['text'].apply(url_replacement)\n",
    "test['text'] = test['text'].apply(user_replacement)\n",
    "\n",
    "test['text'] = train['text'].str.lower()\n",
    "train['text'] = train['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>answer</th>\n",
       "      <th>company</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>url взять кредит тюмень альфа банк</td>\n",
       "      <td>0</td>\n",
       "      <td>alfabank</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>мнение о кредитной карте втб 24 url</td>\n",
       "      <td>0</td>\n",
       "      <td>vtb</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«райффайзенбанк»: снижение ключевой ставки цб ...</td>\n",
       "      <td>0</td>\n",
       "      <td>raiffeisen</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>современное состояние кредитного поведения в р...</td>\n",
       "      <td>0</td>\n",
       "      <td>sberbank</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user user главное чтоб банки сбер и втб!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>sberbank</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  answer     company  \\\n",
       "0                 url взять кредит тюмень альфа банк       0    alfabank   \n",
       "1                мнение о кредитной карте втб 24 url       0         vtb   \n",
       "2  «райффайзенбанк»: снижение ключевой ставки цб ...       0  raiffeisen   \n",
       "3  современное состояние кредитного поведения в р...       0    sberbank   \n",
       "4         user user главное чтоб банки сбер и втб!!!       1    sberbank   \n",
       "\n",
       "   sample_id  \n",
       "0          1  \n",
       "1          2  \n",
       "2          3  \n",
       "3          4  \n",
       "4          5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to read from the .json.gzip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon.data import ArrayDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = list(\"абвеёжзклмнопрдйгсуфхцчшщъыьэюяит0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\") # The characters as specified in the paper\n",
    "ALPHABET_INDEX = {letter: index for index, letter in enumerate(ALPHABET)} # { a: 0, b: 1, etc}\n",
    "FEATURE_LEN = 157 # max-length in characters for one document\n",
    "NUM_WORKERS = max(multiprocessing.cpu_count() - 3, 1)# number of workers used in the data loading\n",
    "BATCH_SIZE = 4000 # number of documents per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    encoded = np.zeros([len(ALPHABET), FEATURE_LEN], dtype='float32')\n",
    "    review = text.lower()[:FEATURE_LEN-1:-1]\n",
    "    i = 0\n",
    "    for letter in text:\n",
    "        if i >= FEATURE_LEN:\n",
    "            break;\n",
    "        if letter in ALPHABET_INDEX:\n",
    "            encoded[ALPHABET_INDEX[letter]][i] = 1\n",
    "        i += 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x, y):\n",
    "    return encode(x), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_X = train['text'].as_matrix()\n",
    "train_data_Y = train['answer'].as_matrix()\n",
    "test_data_X = test['text'].as_matrix()\n",
    "test_data_Y = test['answer'].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ArrayDataset(train_data_X, train_data_Y).transform(transform)\n",
    "test_dataset = ArrayDataset(test_data_X, test_data_Y).transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training and testing dataloader, with NUM_WORKERS set to the number of CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4000\n",
    "train_data_loader = mx.gluon.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_data_loader = mx.gluon.data.DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context will define where the training takes place, on the CPU or on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the network following the instructions describe in the paper, using the small feature and small output units configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](data/diagram.png)\n",
    "![img](data/convolutional_layers.png)\n",
    "![img](data/dense_layer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper we set the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 256 # number of convolutional filters per convolutional layer\n",
    "NUM_OUTPUTS = 2 # number of classes\n",
    "FULLY_CONNECTED = 1024 # number of unit in the fully connected dense layer\n",
    "DROPOUT_RATE = 0.5 # probability of node drop out\n",
    "LEARNING_RATE = 0.01 # learning rate of the gradient\n",
    "MOMENTUM = 0.04 # momentum of the gradient\n",
    "WDECAY = 0.0001 # regularization term to limit size of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.HybridSequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(NUM_OUTPUTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): Conv1D(None -> 256, kernel_size=(7,), stride=(1,), Activation(relu))\n",
      "  (1): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (2): Conv1D(None -> 256, kernel_size=(7,), stride=(1,), Activation(relu))\n",
      "  (3): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (4): Conv1D(None -> 256, kernel_size=(3,), stride=(1,), Activation(relu))\n",
      "  (5): Conv1D(None -> 256, kernel_size=(3,), stride=(1,), Activation(relu))\n",
      "  (6): Conv1D(None -> 256, kernel_size=(3,), stride=(1,), Activation(relu))\n",
      "  (7): Conv1D(None -> 256, kernel_size=(3,), stride=(1,), Activation(relu))\n",
      "  (8): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (9): Flatten\n",
      "  (10): Dense(None -> 1024, Activation(relu))\n",
      "  (11): Dropout(p = 0.5, axes=())\n",
      "  (12): Dense(None -> 1024, Activation(relu))\n",
      "  (13): Dropout(p = 0.5, axes=())\n",
      "  (14): Dense(None -> 2, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybridize = False # for speed improvement, compile the network but no in-depth debugging possible\n",
    "load_params = False # Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybridization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hybridize:\n",
    "    net.hybridize(static_alloc=True, static_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cross-entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in a multi-class classification problem, so we use the [Softmax Cross entropy loss](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "                        {'learning_rate': LEARNING_RATE, \n",
    "                         'wd':WDECAY, \n",
    "                         'momentum':MOMENTUM})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        prediction = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=prediction, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "We loop through the batches given by the data_loader. These batches have been asynchronously fetched by the workers.\n",
    "\n",
    "After an epoch, we measure the test_accuracy and save the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.6934, Test_acc 0.6776\n",
      "Epoch 1. Loss: 0.6740, Test_acc 0.6776\n",
      "Epoch 2. Loss: 0.6555, Test_acc 0.6776\n",
      "Epoch 3. Loss: 0.6381, Test_acc 0.6776\n",
      "Epoch 4. Loss: 0.6206, Test_acc 0.6776\n",
      "Epoch 5. Loss: 0.6040, Test_acc 0.6776\n",
      "Epoch 6. Loss: 0.5892, Test_acc 0.6776\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "number_epochs = 7\n",
    "smoothing_constant = .01\n",
    "for e in range(start_epoch, number_epochs):\n",
    "    for i, (review, label) in enumerate(train_data_loader):\n",
    "        review = review.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(review)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(review.shape[0])\n",
    "        \n",
    "        # moving average of the loss\n",
    "        curr_loss = nd.mean(loss)\n",
    "        moving_loss = (curr_loss if (i == 0) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "        #if (i%200 == 0):\n",
    "            #print('Batch {}: Instant loss {:.4f}, Moving loss {:.4f}'.format(i,curr_loss.asscalar(), moving_loss.asscalar()))\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data_loader, net)\n",
    "    #Save the model using the gluon params format\n",
    "    #net.save_parameters('crepe_epoch_{}_test_acc_{}.params'.format(e,int(test_accuracy*10000)/100))\n",
    "    print(\"Epoch {}. Loss: {:.4f}, Test_acc {:.4f}\".format(e, moving_loss.asscalar(), test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.export('crepe', epoch=number_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
